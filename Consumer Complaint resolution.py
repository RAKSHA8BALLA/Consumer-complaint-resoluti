# -*- coding: utf-8 -*-
"""DMX2_RB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qo1TdgCBao7t4dsYNzO3dnrWFw4_uMav
"""

import pandas as pd
import datetime as dt
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

te_file_path = 'sample_data/Consumer_Complaints_test.csv'
tr_file_path = 'sample_data/Consumer_Complaints_train.csv'
# read the data and store data in DataFrame titled melbourne_data
tr_data = pd.read_csv(tr_file_path)
te_data = pd.read_csv(te_file_path)

tr_data.describe()

tr_data.info()

te_data.info()

tr_data.any()

tr_data.isnull().any()

tr_data.head()

te_data.any()

te_data.isnull().any()

nan_count = tr_data.isna().sum()
print(nan_count)

nan_count = te_data.isna().sum()
print(nan_count)

min_count =  int(((100-25)/100)*te_data.shape[0] + 1)
mod_df = te_data.dropna( axis=1, thresh=min_count)
print(mod_df)

min_count =  int(((100-25)/100)*tr_data.shape[0] + 1)
mod_df = tr_data.dropna( axis=1, thresh=min_count)
print(mod_df)

percent_missing = tr_data.isnull().sum() * 100 / len(tr_data)
missing_value_df = pd.DataFrame({'column_name': tr_data.columns,
                                 'percent_missing': percent_missing})
print(percent_missing)

mod_df = tr_data.dropna(axis=1, thresh= percent_missing)
print(tr_data)

percent_missing = te_data.isnull().sum() * 100 / len(te_data)
missing_value_df = pd.DataFrame({'column_name': te_data.columns,
                                 'percent_missing': percent_missing})
print(percent_missing)

mod_df = te_data.dropna(axis=1, thresh= percent_missing)
print(te_data)

tr_data.head()

te_data.head()

te_data.isnull().any()

nan_count = te_data.isna().sum()
print(nan_count)

tr_data['Date received'] = pd.to_datetime(tr_data['Date received'])
tr_data.dtypes
tr_data['Date sent to company'] = pd.to_datetime(tr_data['Date sent to company'])
tr_data.dtypes

te_data['Date received'] = pd.to_datetime(te_data['Date received'])
te_data.dtypes

tr_data['Days Held']=tr_data['Date sent to company']-tr_data['Date received']
tr_data.head()

te_data['Days held']=te_data['Date sent to company'].apply(pd.to_datetime)-te_data['Date received'].apply(pd.to_datetime)

#te_data['Data Held']=te_data['Date sent to company']-te_data['Date received']
te_data.head()

#data_held = tr_data['Data Held']

tr_data.info()

#data_held = te_data['Data Held']

tr_data.head()

#tr_data.drop(['Date Received', 'Date Sent to Company', 'ZIP Code', 'Complaint ID'], axis=1)

tr_data.drop(['Date received','Date sent to company','ZIP code', 'Complaint ID'], axis=1)

te_data.drop(['Date received','Date sent to company','ZIP code','Complaint ID'], axis=1)

#tr_data.info()

tr_data = tr_data.fillna(tr_data['State'].mode())
tr_data.head()

te_data = te_data.fillna(te_data['State'].mode())
#te_data.head()

tr_data['State']= tr_data.fillna(tr_data['State'].mode(),inplace=True)
#tr_data.head()

te_data['State']=te_data.fillna(te_data['State'].mode(),inplace=True)
#te_data.head()

tr_data['State'].unique()
#tr_data.head()

#tr_data['Data Held']=tr_data['Data Held'].apply(pd.to_datetime)
#tr_data["Weeks"] = tr_data['Data Held'].dt.week

#tr_data['Date'] = pd.to_datetime(dict(year=tr_data['Year'], month=tr_data['Month'], day=tr_data['Day']))

disputed_cons = tr_data['Consumer disputed?']
tr_data['Disputed_cons'] = disputed_cons
#tr_data.info()

#te_data.head()

#tr_data

tr_data.describe()

#tr_data.info()

"""drop negative days held

"""

import numpy as np

tr_data['Days Held'] = tr_data['Days Held'].astype('int')

#No_of_days = tr_data['Days Held'].abs
#week_received = tr_data(['Days Held']/7)
#print(No_of_days//7)

tr_data[tr_data['Days Held'] < 0] = 0

#te_data

#te_data['Days held'] =te_data['Days held'].dt.days

te_data['Days held'] = te_data['Days held'].astype('int')

te_data[te_data['Days held'] < 0] = 0
#te_data.head()

tr_data.info()

!pip install textblob
!pip install nltk

import nltk
import string
nltk.download("stopwords")
nltk.download("wordnet")
nltk.download("omw-1.4")
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
sw = stopwords.words("english")
from nltk.corpus import wordnet as wn
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.stem import PorterStemmer
from nltk.tokenize import wordpunct_tokenize

tr_data['Issue'] = tr_data['Issue'].astype('str')
te_data['Issue'] = te_data['Issue'].astype('str')

"""Text pre-processing"""

relevant_text_train = tr_data['Issue']
relevant_text_test = te_data['Issue']
tokenized_data_train = relevant_text_train.apply(lambda x: wordpunct_tokenize(x.lower()))
tokenized_data_test = relevant_text_test.apply(lambda x: wordpunct_tokenize(x.lower()))
def remove_punctuation(text):
    no_punctuation = []
    for w in text:
        if w not in string.punctuation:
            no_punctuation.append(w)
    return no_punctuation
no_punctuation_data_train = tokenized_data_train.apply(lambda x: remove_punctuation(x))
no_punctuation_data_test = tokenized_data_test.apply(lambda x: remove_punctuation(x))
stop_words = stopwords.words('english')
filtered_sentence_train = [w for w in no_punctuation_data_train if not w in stop_words]
filtered_sentence_train = pd.Series(filtered_sentence_train)
filtered_sentence_test = [w for w in no_punctuation_data_test if not w in stop_words]
filtered_sentence_test = pd.Series(filtered_sentence_test)
def lemmatize_text(text):
    lem_text = [WordNetLemmatizer().lemmatize(w,pos = 'v') for w in text]
    return lem_text
lemmatized_data_train = filtered_sentence_train.apply(lambda x:lemmatize_text(x))
lemmatized_data_test = filtered_sentence_test.apply(lambda x:lemmatize_text(x))
def stem_text(text):
    stem_text = [PorterStemmer().stem(w) for w in text]
    return stem_text
stemmed_data_train = lemmatized_data_train.apply(lambda x:stem_text(x))
stemmed_data_test = lemmatized_data_test.apply(lambda x:stem_text(x))
def word_to_sentence(text):
    text_sentence = " ".join(text)
    return text_sentence
clean_data_train = stemmed_data_train.apply(lambda x:word_to_sentence(x))
clean_data_test = stemmed_data_test.apply(lambda x:word_to_sentence(x))

tr_data['Issues_cleaned'] = clean_data_train
te_data['Issues_cleaned'] = clean_data_test
df_train = tr_data.drop('Issue', axis = 1)
df_test = te_data.drop('Issue', axis = 1)

tr_data['Issues_cleaned'] = clean_data_train
tr_data = tr_data.drop('Issue', axis = 1)

te_data['Issues_cleaned'] = clean_data_test
te_data = te_data.drop('Issue', axis = 1)

tr_data.drop(['Company', 'State'], axis=1)

te_data.drop(['Company', 'State'], axis=1)

#tr_data.head()

#tr_data['Consumer disputed?'] = tr_data['Consumer disputed?'].map({'Yes': 1, 'No': 0})

#tr_data.head()

#tr_data.head()

#tr_data['Timely response?'] = tr_data['Timely response?'].map({'Yes': 1, 'No': 0})

#te_data['Timely response?'] = te_data['Timely response?'].map({'Yes': 1, 'No': 0})

#te_data.head()

print(tr_data["Product"].unique())

print(tr_data["Submitted via"].unique())

print(tr_data["Company response to consumer"].unique())

#import pandas as pd
#from sklearn.preprocessing import StandardScaler
#from sklearn.decomposition import PCA

#import pandas as pd

# Load the dataset into a pandas DataFrame
#data = pd.read_csv('dataset.csv')

# Print the column names of the DataFrame
print(tr_data.columns)

# Select the column that contains the dependent variable
dependent_variable = tr_data['Consumer disputed?']

# Print the first few rows of the dependent variable
print(dependent_variable.head())

#tr_data

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(data=tr_data, x='Consumer disputed?')

sns.countplot(data=tr_data, x='Consumer disputed?')

fig,ax = plt.subplots(figsize=(15,7))
sns.countplot(x='Product',data=tr_data)

fig,ax = plt.subplots(figsize=(15,7))
sns.countplot(x='Product',data=te_data)

tr_data.head()

fig,ax = plt.subplots(figsize=(15,7))
sns.countplot(x='Submitted via',data=te_data)

fig,ax = plt.subplots(figsize=(15,7))
sns.countplot(x='Submitted via',data=tr_data)

fig,ax = plt.subplots(figsize=(15,7))
sns.countplot(x='Company response to consumer',data=tr_data)

fig,ax = plt.subplots(figsize=(15,7))
sns.countplot(x='Company response to consumer',data=te_data)

#fig,ax = plt.subplots(figsize=(15,7))
#sns.barplot(x='Timely response?',y='Consumer disputed?',data=te_data)

#sns.countplot(x=tr_data[tr_data['Consumer disputed?'] == 'Yes']['Issue'],data=tr_data)

#fig,ax = plt.subplots(figsize=(22,6))
#sns.countplot(x=tr_data[tr_data['Consumer disputed?'] == 'Yes']['Issue'],data=tr_data)
tr_data

sns.countplot(x='Timely response?',data=tr_data)

#tr_data.head()

#tr_data["Issue"].unique()

#tr_data.head()

#tr_data.head()

tr_data['Sub-issue'].unique()

##tr_data['Issue'] = tr_data.index
#fig,ax = plt.subplots(figsize=(15,7))
#sns.barplot(x='Issue',data=tr_data)
#tr_data['Issue']

#fig,ax = plt.subplots(figsize=(33,8))
#sns.countplot(x=tr_data[tr_data['Consumer disputed?'] == 'Yes']['Issue'],data=tr_data)
#tr_data.info()

import seaborn as sns

sns.countplot(x=tr_data[tr_data['Consumer disputed?'] == 'Yes']['Timely response?'] ,data=tr_data)

sns.countplot(x=tr_data[tr_data['Consumer disputed?'] == 'Yes']['Company response to consumer'],data=tr_data)

#tr_data.info()

fig,ax = plt.subplots(figsize=(33,8))
sns.countplot(x=tr_data[tr_data['Consumer disputed?'] == 'Yes']['Company response to consumer'],data=tr_data)

sns.countplot(x=tr_data[tr_data['Consumer disputed?'] == 'Yes']['Submitted via'],data=tr_data)

fig,ax = plt.subplots(figsize=(33,8))
sns.countplot(x=tr_data[tr_data['Consumer disputed?'] == 'Yes']['Company'],data=tr_data)

#tr_data

"""Drop Unnecessary Columns for the Model Building<br>
like:'Company', 'State', 'Year_Received', 'Days_held'
"""

tr_data = tr_data.drop(['Company','State','Days Held'], axis = 1)

te_data = te_data.drop(['Company','State','Days held'], axis = 1)

"""Change Consumer Disputed Column to 0 and 1(yes to 1, and no to 0)"""

tr_data["Consumer disputed?"] = np.where(tr_data["Consumer disputed?"]=="Yes",1,0)
tr_data.head()

df_dummies=pd.get_dummies(tr_data, prefix=['Product_dumb', 'Submitted via_dumb', 'Company response to consumer_dumb', 'Timely response?_dumb'], columns=['Product', 'Submitted via', 'Company response to consumer', 'Timely response?'])
tr_data.head()

df_dummies= df_dummies.drop(['Issues_cleaned'],axis=1)
df_dummies

df_dummies_test=pd.get_dummies(te_data, prefix=['Product_dumb', 'Submitted via_dumb', 'Company response to consumer_dumb', 'Timely response?_dumb'], columns=['Product', 'Submitted via', 'Company response to consumer', 'Timely response?'])

df_dummies_test= df_dummies_test.drop(['Issues_cleaned'],axis=1)
df_dummies_test

"""Concate Dummy Variables and Drop the Original Columns"""

tr_data =pd.concat([tr_data,df_dummies],axis=1)
tr_data.head()

#tr_data = tr_data.drop(['Product','Submitted via','Company response to consumer','Timely response?'], axis = 1)
#tr_data

#te_data =pd.concat([te_data,df_dummies_test],axis=1)

#te_data = te_data.drop(['Product','Submitted via','Company response to consumer','Timely response?'], axis = 1)

"""calculating tfidf"""

#from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
#tf = TfidfVectorizer()
#issues_cleaned_train = tf.fit_transform(tr_data['Issues_cleaned']).toarray()
#issues_cleaned_test = tf.fit_transform(te_data['Issues_cleaned']).toarray()
#tf_columns_train = []
#tf_columns_test = []
#for i in range(issues_cleaned_train.shape[1]):
 #   tf_columns_train.append('Feature' + str(i+1))
#for i in range(issues_cleaned_test.shape[1]):
 #   tf_columns_test.append('Feature' + str(i+1))
#issues_train = pd.DataFrame(issues_cleaned_train, columns = tf_columns_train)
#issues_test = pd.DataFrame(issues_cleaned_test, columns = tf_columns_test)
#weights = pd.DataFrame(tf.idf_, index = tf.get_feature_names_out(), columns = ['Idf_weights']).sort_values(by = 'Idf_weights', ascending = False)
#weights.head()

#tr_data = tr_data.drop('Issues_cleaned', axis = 1)
#te_data = te_data.drop('Issues_cleaned', axis = 1)
#tr_data = pd.concat([tr_data, issues_train], axis = 1)
#te_data = pd.concat([te_data, issues_test], axis = 1)
#Feature168 = [0] * 119606
#te_data['Feature168'] = Feature168

"""observe shapee of new train and test datasets"""

tr_data.shape

te_data.shape

df_copy_test=te_data.copy()
df_copy_test=df_copy_test.drop(['Dummy'],axis=1)

#df_copy_train=tr_data.copy()
#df_copy_train=df_copy_train.drop(['Dummy'],axis=1)

"""Scaling the Data Sets (note:discard dependent variable before doing standardization)"""

from sklearn.preprocessing import StandardScaler
scale = StandardScaler()
scaledX_test = scale.fit_transform(df_copy_test)
print(scaledX_test)

from sklearn.preprocessing import StandardScaler
scale = StandardScaler()
scaledX_train = scale.fit_transform(df_copy_train)
print(scaledX_train)

"""Do feature selection with help of PCA"""

from sklearn import decomposition
pca = decomposition.PCA()
scaledX_train_pca = pca.fit_transform(scaledX_train)
print(scaledX_train_pca)